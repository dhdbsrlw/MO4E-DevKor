# -*- coding: utf-8 -*-
"""pykrx.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PqNlByyA75zZqXryqZwAfOSdvRH-YJ3b
"""

pip install pykrx

from pykrx import stock

# tickers = stock.get_market_ticker_list(조회일자(YYYYmmdd) [,market=조회할 시장(KOSPI, KOSDAQ, ALL])
# name = get_market_ticker_name(심볼 [,market=조회할 시장(KOSPI, KOSDAQ, ALL])
# ohlcv = stock.stock.get_market_ohlcv(조회일자(YYYYmmdd) [,앞에 시작일을 넣었다면 종료일] [,심볼] )

# 예시
tickers = stock.get_market_ticker_list("20230925", market="KOSPI") # 코스피 종목 리스트
name = stock.get_market_ticker_name("005930") # 기업 이름
ohlcv1 = stock.get_market_ohlcv("20230925", market="KOSPI") # 특정일 코스피 종목들의 OHLCV
ohlcv2 = stock.get_market_ohlcv("20230901", "20230925", "005930") # 특정기간 특정 종목의 OHLCV

# 예시출력 1
tickers[:10]

# 예시출력 (기업이름 for 종목번호(심볼) 005930)
name

# 예시출력 3 (여러 종목에 대해)
ohlcv1[:10]

# 예시출력 4 (한 종목에 대해 날짜별 - 삼성전자)
ohlcv2[:10]

"""# Price Prediction (Project Start)
Reference: https://www.kaggle.com/code/raoulma/ny-stock-price-prediction-rnn-lstm-gru

1. Setting
"""

import numpy as np
import pandas as pd
import math
import sklearn
import sklearn.preprocessing
import datetime
import os
import matplotlib.pyplot as plt
import tensorflow as tf

# split data in 80%/10%/10% (train/validation/test)
valid_set_size_percentage = 10
test_set_size_percentage = 10

"""2. Analyze Data"""

# 주가 데이터 샘플

df  = stock.get_market_ohlcv("20230101", "20230925", "005930")
# df = stock.get_market_ohlcv("20230925", market="KOSPI")
df.info()
df.head()

# Draw Figure

plt.figure(figsize=(15, 5));
plt.subplot(1,2,1);
plt.plot(df.시가.values, color='red', label='open')
plt.plot(df.종가.values, color='green', label='close')
plt.plot(df.저가.values, color='blue', label='low')
plt.plot(df.고가.values, color='black', label='high')
plt.title('stock price')
plt.xlabel('time [days]')
plt.ylabel('price')
plt.legend(loc='best')
plt.show()

# column 명 변경
df.columns = ['open', 'high', 'low', 'close', 'volume', 'RoC']

df.head()

"""# 3. Manipulate Data


*  choose a specific stock
*  drop feature: volume
*  normalize stock data
*  create train, validation and test data sets








"""

# function for min-max normalization of stock

def normalize_data(df):
    min_max_scaler = sklearn.preprocessing.MinMaxScaler()
    df['open'] = min_max_scaler.fit_transform(df.open.values.reshape(-1,1))
    df['high'] = min_max_scaler.fit_transform(df.high.values.reshape(-1,1))
    df['low'] = min_max_scaler.fit_transform(df.low.values.reshape(-1,1))
    df['close'] = min_max_scaler.fit_transform(df['close'].values.reshape(-1,1))
    return df

# function for crate train/val/test data given stock data and sequence length

def load_data(stock, seq_len):
    data_raw = stock.values # stock.as_matrix() # convert to numpy array
    data = []

    # create all possible sequences of length seq_len
    for index in range(len(data_raw) - seq_len):
        data.append(data_raw[index: index + seq_len])

    data = np.array(data);
    valid_set_size = int(np.round(valid_set_size_percentage/100*data.shape[0]));
    test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]));
    train_set_size = data.shape[0] - (valid_set_size + test_set_size);

    print(data)
    x_train = data[:train_set_size,:-1,:]
    y_train = data[:train_set_size,-1,:]

    x_valid = data[train_set_size:train_set_size+valid_set_size,:-1,:]
    y_valid = data[train_set_size:train_set_size+valid_set_size,-1,:]

    x_test = data[train_set_size+valid_set_size:,:-1,:]
    y_test = data[train_set_size+valid_set_size:,-1,:]

    return [x_train, y_train, x_valid, y_valid, x_test, y_test]

# choose one stock
df_stock = df.copy()
df_stock.drop(['RoC'],1,inplace=True)
df_stock.drop(['volume'],1,inplace=True)

cols = list(df_stock.columns.values)
print('df_stock.columns.values = ', cols)

# normalize stock
df_stock_norm = df_stock.copy()
df_stock_norm = normalize_data(df_stock_norm)

df_stock_norm # for debugging

temp = df_stock_norm.values
print(temp) # for debugging

len(temp) # for debugging

# create train, test data
seq_len = 20 # choose sequence length
x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df_stock_norm, seq_len)
print('x_train.shape = ',x_train.shape)
print('y_train.shape = ', y_train.shape)
print('x_valid.shape = ',x_valid.shape)
print('y_valid.shape = ', y_valid.shape)
print('x_test.shape = ', x_test.shape)
print('y_test.shape = ',y_test.shape)

# plot fig

plt.figure(figsize=(15, 5));
plt.plot(df_stock_norm.open.values, color='red', label='open')
plt.plot(df_stock_norm.close.values, color='green', label='low')
plt.plot(df_stock_norm.low.values, color='blue', label='low')
plt.plot(df_stock_norm.high.values, color='black', label='high')
#plt.plot(df_stock_norm.volume.values, color='gray', label='volume')
plt.title('stock')
plt.xlabel('time [days]')
plt.ylabel('normalized price/volume')
plt.legend(loc='best')
plt.show()

"""## Model and Validate Data

RNNs with basic, LSTM, GRU
"""

# 삭제 예정
index_in_epoch = 0;
perm_array  = np.arange(x_train.shape[0]) # Numpy Array 화
perm_array

# tensorflow 대체 목적

import torch
import torch.nn as nn
import torch.optim as optim

## Basic Cell RNN in tensorflow (Pytorch 로 대체 필요)

index_in_epoch = 0;
perm_array  = np.arange(x_train.shape[0]) # Numpy Array 화
np.random.shuffle(perm_array)

# function to get the next batch
def get_next_batch(batch_size):
    global index_in_epoch, x_train, perm_array
    start = index_in_epoch
    index_in_epoch += batch_size

    if index_in_epoch > x_train.shape[0]:
        np.random.shuffle(perm_array) # shuffle permutation array
        start = 0 # start next epoch
        index_in_epoch = batch_size

    end = index_in_epoch
    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]

# parameters
n_steps = seq_len-1
n_inputs = 4
n_neurons = 200
n_outputs = 4
n_layers = 2
learning_rate = 0.001
batch_size = 50
n_epochs = 100
train_set_size = x_train.shape[0]
test_set_size = x_test.shape[0]

"""이하는, 레퍼런스의 텐서플로우 코드를 파이토치 코드를 변환한 것이다."""

# Define the RNN model
class RNNModel(nn.Module):
    def __init__(self, n_inputs, n_neurons, n_layers, n_outputs):
        super(RNNModel, self).__init__()
        self.rnn = nn.RNN(input_size=n_inputs, hidden_size=n_neurons, num_layers=n_layers, nonlinearity='relu')
        self.fc = nn.Linear(n_neurons, n_outputs)

    def forward(self, x):
        rnn_outputs, _ = self.rnn(x)
        stacked_rnn_outputs = rnn_outputs.view(-1, self.rnn.hidden_size)
        stacked_outputs = self.fc(stacked_rnn_outputs)
        outputs = stacked_outputs.view(-1, n_steps, n_outputs)[:, -1, :]  # Keep only the last output
        return outputs

# Create the RNN model
n_steps = seq_len - 1  # Assuming seq_len is defined
n_inputs = 4
n_neurons = 200
n_outputs = 4
n_layers = 2
learning_rate = 0.001
batch_size = 50
n_epochs = 100
train_set_size = x_train.shape[0]
test_set_size = x_test.shape[0]

model = RNNModel(n_inputs, n_neurons, n_layers, n_outputs)

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Convert training data to PyTorch tensors
X_train = torch.Tensor(x_train)
y_train = torch.Tensor(y_train)

# Training loop
for epoch in range(n_epochs):
    for iteration in range(0, train_set_size, batch_size):
        x_batch = X_train[iteration:iteration + batch_size]
        y_batch = y_train[iteration:iteration + batch_size]

        optimizer.zero_grad()  # Zero the gradients
        outputs = model(x_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()  # Compute gradients
        optimizer.step()  # Update weights

    # Calculate and print the training and validation loss
    mse_train = criterion(model(X_train), y_train)
    mse_valid = criterion(model(torch.Tensor(x_valid)), torch.Tensor(y_valid))
    print(f"Epoch {epoch + 1}: MSE train/valid = {mse_train.item():.6f}/{mse_valid.item():.6f}")

# Perform predictions on the training, validation, and test sets
y_train_pred = model(X_train).detach().numpy()
y_valid_pred = model(torch.Tensor(x_valid)).detach().numpy()
y_test_pred = model(torch.Tensor(x_test)).detach().numpy()

y_train_pred.shape

ft = 0 # 0 = open, 1 = high, 2 = low, 3 = close

## show predictions
plt.figure(figsize=(15, 5));
plt.subplot(1,2,1);

plt.plot(np.arange(y_train.shape[0]), y_train[:,ft], color='blue', label='train target')
plt.plot(np.arange(y_train.shape[0], y_train.shape[0]+y_valid.shape[0]), y_valid[:,ft],
         color='gray', label='valid target')
plt.plot(np.arange(y_train.shape[0]+y_valid.shape[0],
                   y_train.shape[0]+y_test.shape[0]+y_test.shape[0]),
         y_test[:,ft], color='black', label='test target')
plt.plot(np.arange(y_train_pred.shape[0]),y_train_pred[:,ft], color='red',
         label='train prediction')
plt.plot(np.arange(y_train_pred.shape[0], y_train_pred.shape[0]+y_valid_pred.shape[0]),
         y_valid_pred[:,ft], color='orange', label='valid prediction')
plt.plot(np.arange(y_train_pred.shape[0]+y_valid_pred.shape[0],
                   y_train_pred.shape[0]+y_valid_pred.shape[0]+y_test_pred.shape[0]),
         y_test_pred[:,ft], color='green', label='test prediction')

plt.title('past and future stock prices')
plt.xlabel('time [days]')
plt.ylabel('normalized price')
plt.legend(loc='best');

plt.subplot(1,2,2);

plt.plot(np.arange(y_train.shape[0], y_train.shape[0]+y_test.shape[0]),
         y_test[:,ft], color='black', label='test target')

plt.plot(np.arange(y_train_pred.shape[0], y_train_pred.shape[0]+y_test_pred.shape[0]),
         y_test_pred[:,ft], color='green', label='test prediction')

plt.title('future stock prices')
plt.xlabel('time [days]')
plt.ylabel('normalized price')
plt.legend(loc='best');

# corr_price_development_train = (torch.sum((torch.sign(y_train[:, 1] - y_train[:, 0]) == torch.sign(y_train_pred[:, 1] - y_train_pred[:, 0])).int()) / y_train.shape[0]).item()
# corr_price_development_valid = (torch.sum((torch.sign(y_valid[:, 1] - y_valid[:, 0]) == torch.sign(y_valid_pred[:, 1] - y_valid_pred[:, 0])).int()) / y_valid.shape[0]).item()
# corr_price_development_test = (torch.sum((torch.sign(y_test[:, 1] - y_test[:, 0]) == torch.sign(y_test_pred[:, 1] - y_test_pred[:, 0])).int()) / y_test.shape[0]).item()

# 이하 오류 코드
# corr_price_development_train = np.sum(np.equal(np.sign(y_train[:,1]-y_train[:,0]),
#             np.sign(y_train_pred[:,1]-y_train_pred[:,0])).astype(int)) / y_train.shape[0]
# corr_price_development_valid = np.sum(np.equal(np.sign(y_valid[:,1]-y_valid[:,0]),
#             np.sign(y_valid_pred[:,1]-y_valid_pred[:,0])).astype(int)) / y_valid.shape[0]
# corr_price_development_test = np.sum(np.equal(np.sign(y_test[:,1]-y_test[:,0]),
#             np.sign(y_test_pred[:,1]-y_test_pred[:,0])).astype(int)) / y_test.shape[0]

# print('correct sign prediction for close - open price for train/valid/test: %.2f/%.2f/%.2f'%(
#     corr_price_development_train, corr_price_development_valid, corr_price_development_test))